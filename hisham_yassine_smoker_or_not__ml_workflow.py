# -*- coding: utf-8 -*-
"""Hisham Yassine -  Smoker or not? - ML Workflow.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Cks03BqdzlpLnTb1wSwXc1dr6vlmhjeL

# **Challenge of the Week - ML Workflow** 

© 2022 Zaka AI, Inc. All Rights Reserved.

---

## Case Study: Classifying Smokers vs Non Smokers

**Objective:**

In this challenge, you will work on a dataset that you saw during week 1 but this time with different aspect. Your goal is not only to make a prediction, it is to make a prediction with the best possible way. So you will be building, evaluating, and improving your model.


## Dataset Description


*   **age**: age of primary beneficiary
*   **sex**: insurance contractor gender, female, male
*   **bmi**: Body mass index, providing an understanding of body, weights that are relatively high or low relative to height,
objective index of body weight (kg / m ^ 2) using the ratio of height to weight, ideally 18.5 to 24.9
*   **children**: Number of children covered by health insurance / Number of dependents
*   **smoker**: Smoking
*   **region**: the beneficiary's residential area in the US, northeast, southeast, southwest, northwest.
*   **charges**: Individual medical costs billed by health insurance

Our problem would be to predict if a person is smoker or not based on all the other features in the dataset.

## 1. Data Loading

#### Import necessary python modules

We will need the following libraries:
 - Numpy — for scientific computing (e.g., linear algebra (vectors & matrices)).
 - Pandas — providing high-performance, easy-to-use data reading, manipulation, and analysis.
 - Matplotlib — plotting & visualization.
 - scikit-learn — a tool for data mining and machine learning models. We need from it train_test_split and LogisticRegression Functions
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression

"""#### Read & visualize data
To load the data to our code, we use **pandas** module, more specifically, the **read_csv** function. Print the head of your dataset.
"""

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv('/content/drive/MyDrive/Zaka - AIC/Week 4/Weekly Challenge 4/insurance.csv')
df.head()

"""## 2. Exploratory Data Analysis

Let's dig deeper & understand our data

**Task:** how many rows & columns in our dataset
"""

print("Our data consists of",len(df),"rows and", len(df.columns),"columns")

"""Using the function **info()**, we can check:
 - data types (int, float, or object (e.g., string))
 - missing values
 - memory usage
 - number of rows and columns
"""

df.info()

"""Using the function **describe()**, we can check the mean, standard deviation, maximum, and minimum of each numerical feature (column)"""

df.describe()

"""#### Data Imbalance Checking

First, let's see how many smokers vs non-smokers we have.
"""

smokers = df[df['smoker']=='yes']
non_smokers = df[df['smoker']=='no']

print("There are",smokers.shape[0],"smokers and", non_smokers.shape[0], "non-smokers")

"""We have an imbalance that we will fix later.

Let's see how much each feature tells us about a person being  a smoker or not.
For each of your numerical features, plot the distribution for the smokers and the non smokers case.
"""

plt.figure(figsize=(18,10))

plt.subplot(221)
bmi_smokers=smokers['bmi'].values
bmi_non_smokers=non_smokers['bmi'].values
plt.hist(bmi_smokers, color='blue',alpha=0.5, label='Smokers')
plt.hist(bmi_non_smokers, color='orange',alpha=0.5, label='Non-Smokers')
plt.title("BMI Distribution")
plt.legend()

plt.subplot(222)
age_smokers=smokers['age'].values
age_non_smokers=non_smokers['age'].values
plt.hist(age_smokers, color='blue', alpha=0.5, label='Smokers')
plt.hist(age_non_smokers, color='orange', alpha=0.5, label='Non-Smokers')
plt.title("Age Distribution")
plt.legend()


plt.subplot(223)
children_smokers=smokers['children'].values
children_non_smokers=non_smokers['children'].values
plt.hist(children_smokers, color='blue', alpha=0.5, label='Smokers')
plt.hist(children_non_smokers, color='orange', alpha=0.5, label='Non-Smokers')
plt.title("Children Distribution")
plt.legend()

plt.subplot(224)
charges_smokers=smokers['charges'].values
charges_non_smokers=non_smokers['charges'].values
plt.hist(charges_smokers, color='blue', alpha=0.5, label='Smokers')
plt.hist(charges_non_smokers, color='orange', alpha=0.5, label='Non-Smokers')
plt.title("Charges Distribution")
plt.legend()

"""WHat do you think is the feature that tells us the most about a person being smoker or not?

**The medical charges**

Now let's see if the gender influences being a smoker or not.
<br>Hint: You can use crosstabbing and then plot distributions
"""

smokers_per_sex=pd.crosstab(df['sex'],df['smoker'])
smokers_per_sex.plot.bar(stacked=True)

"""Do you think the gender has an effect on smoking according to your results?

**Yes, it seems that male individuals have a higher smoking percentage**

Do the same for the column 'region'
"""

smokers_per_sex=pd.crosstab(df['region'],df['smoker'])
smokers_per_sex.plot.bar(stacked=True)

"""## 3. Data Preprocessing
"Garbage in, garbage out". 

Data should be preprocessed and cleaned to get rid of noisy data. 
Preprocessing includes:
 - dealing with missing data
   - remove whole rows (if they are not a lot)
   - infer (e.g., date of birth & age)
   - fill with mean, median, or even 0
 - removing unsued column(s)
 - convert categorical (non numerical) data into numerical
 - normalization: standarize data ranges for all features (e.g., between 0 and 1)



---



 Let's start by seeing if we have missing data.
"""

df.isnull().sum()

"""Drop rows with missing values, and make sure you have no nulls anymore."""

df= df.dropna()
df.isnull().sum()

"""See the type of each of your columns"""

df.dtypes

"""#### Convert Categorical columns to numerical

*   We need to convert the sex column from male/female to 0/1.
*   We need to convert the smoker column from no/yes to 0/1.


Let's start with the sex column


"""

genders_dic = {'male':0, 'female':1}
df['sex'] = df['sex'].apply(lambda x: genders_dic[x])

"""And now the smokers column"""

smokers_dic = {'no':0, 'yes':1}
df['smoker'] = df['smoker'].apply(lambda x: smokers_dic[x])

"""And now the Region Column"""

from sklearn import preprocessing

encoder = preprocessing.LabelEncoder()
df['region'] = encoder.fit_transform(df['region'])

df.head()

"""#### Normalization

Let's scale all the columns by dividing by the maximum
"""

data_max = df.max()

#storing our dataframe before making significant changes

data=df.copy()

data = data.divide(data_max)
data.head()

"""## 4. Model Training & Testing

#### Data splits

Before training, we need to split data into training (80%) & testing (20%), and fix the random_state parameter to 42 <br>Print the shapes of your training and testing data.
"""

input_data=data.drop(['smoker'], axis=1)
output_data=data['smoker']
X = input_data.values
y = output_data.values  

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print(x_train.shape, y_train.shape)
print(x_test.shape, y_test.shape)

"""#### Logistic Regression Modeling

Define the logistic Regression model, and fit it on the training data
"""

model = LogisticRegression()  
model.fit(x_train, y_train)

"""#### Evaluation

Evaluate your model on the testing data, and compute: Accuracy, Precision, Recall and F1 score
"""

y_pred = model.predict(x_test)

from sklearn.metrics import accuracy_score,f1_score,recall_score,precision_score

print("Accuracy:",accuracy_score(y_test, y_pred),"\nPrecision:",precision_score(y_test, y_pred),"\nRecall:",recall_score(y_test, y_pred),"\nF1 Score:",f1_score(y_test, y_pred))

"""Which metrics you think can be improved?

**Metrics that can be improved: Accuracy, Recall and F1 score**

##5. Model Improvement

Now we will try to improve the model that we built.

####Handle data Imbalance

Plot a histogram that shows the numbers of smokers and non smokers
"""

data.hist(column='smoker')

"""We can see that we have a clearly imbalanced dataset. To handle it, we choose to do 2 steps:
* Oversampling the minority class with a factor of 0.5
* Undersampling the majority class to obtain the same number in the 2 classes
<br>
We do that by using the RandomOverSaampler and RandomUnderSampler from the imblearn library.
"""

from imblearn.over_sampling import RandomOverSampler 
from imblearn.under_sampling import RandomUnderSampler

oversample = RandomOverSampler(sampling_strategy=0.5)
x_balanced, y_balanced = oversample.fit_resample(X, y)

undersample = RandomUnderSampler(sampling_strategy='majority')
x_balanced, y_balanced= undersample.fit_resample(x_balanced, y_balanced)

"""Plot a new histogram on the new data we have."""

z=np.array([y_balanced]).transpose()
data_balanced =np.hstack([x_balanced,z])
data_balanced_df=pd.DataFrame(data_balanced)
data_balanced_df.columns = ['age', 'sex', 'bmi', 'children', 'region', 'charges','smoker']
data_balanced_df.hist(column='smoker')

"""Split the new data you obtained (80%, 20%), and build a Logistic Regression mode, and fit it on this data. Fix the random_state at 42"""

input_balanced_data=data_balanced_df.drop(['smoker'], axis=1)
output_balanced_data=data_balanced_df['smoker']
X_balanced = input_balanced_data.values
y_balanced = output_balanced_data.values  

x_balanced_train, x_balanced_test, y_balanced_train, y_balanced_test = train_test_split(X_balanced, y_balanced, test_size=0.2, random_state=42)


model_balanced = LogisticRegression()  
model_balanced.fit(x_balanced_train, y_balanced_train)

"""Asses your model on the testing data, and Compute the same metrics you computed before"""

y_balanced_pred = model_balanced.predict(x_balanced_test)

print("Accuracy:",accuracy_score(y_balanced_test, y_balanced_pred),"\nPrecision:",precision_score(y_balanced_test, y_balanced_pred),"\nRecall:",recall_score(y_balanced_test, y_balanced_pred),"\nF1 Score:",f1_score(y_balanced_test, y_balanced_pred))

"""We can see how much our scores got better when we balanced our dataset.

####Regularization with Hyperparameter Tuning

We will be tuning the hyperparameter C in the logistic regression model that we used. This hyperparameter represents regularization.
<br><img src="https://equatio-api.texthelp.com/svg/C%5C%20%3D%5C%20%5Cfrac%7B1%7D%7B%5Clambda%7D" alt="C equals 1 over lamda">

The possible values of C will be: [0.1, 1, 10, 100, 1000]

We will also know have to decide what type of regularization we will use: L1, or L2.

Since we have several parameters to tune, we will be doing what we call a GridSearch, to search for the best model having the best pair of hyperparameters.
We will be doing a 5 folds cross validation

Note: Specify the solver='liblinear' in your LogisticRegression model, and this is because other solvers do not support L1 regularization.

Print the best score and the best parameters
"""

from sklearn.model_selection import GridSearchCV

param_grid = {'penalty':['l1','l2'],'C':[0.1,1,10,100,1000],'solver':['liblinear']}
clf= GridSearchCV(LogisticRegression(), param_grid, cv=5)

clf.fit(x_balanced_train, y_balanced_train)
print("Best Score is:",clf.best_score_,"\nBest Parameters are:",clf.best_params_)

"""Let's assess the model's performance in general for the best specified hyperparameters using the same metrics we used earlier."""

improved_model=LogisticRegression(penalty='l1', C=10, solver='liblinear')
improved_model.fit(x_balanced_train, y_balanced_train)
y_pred_improved = improved_model.predict(x_balanced_test)
print("Accuracy:",accuracy_score(y_balanced_test, y_pred_improved),"\nPrecision:",precision_score(y_balanced_test, y_pred_improved),"\nRecall:",recall_score(y_balanced_test, y_pred_improved),"\nF1 Score:",f1_score(y_balanced_test, y_pred_improved))

"""We can see that the results improved compared to the last model we built.

####AdaBoost Classifier

Now, we will work on improving our model, but this time through using ensemble methods, and what we will use for today is Adaboost.

AdaBoost is a boosting ensemble method that relies on the boosting concepts that we learned about during the week. 
In Adaboost, each model is learning to correct the previous model's mistakes (misclassified examples). After that, each model will have a weight that depends on how well it performed on the training data. And after that, different results are aggregated based on weighted averaging.

Create an AdaBoostClassifier Model from sklearn library with n_estimators=10, and fit it on the training data you have.
"""

#Dear Fouad :)
#I have used the default DecisionTree estimator in this adaboost model since the adaboosted LogisticRegression gave us very low results according to our metrics used.
from sklearn.ensemble import AdaBoostClassifier
adb_model=AdaBoostClassifier(n_estimators=10)
adb_model.fit(x_balanced_train, y_balanced_train)

"""Asses the performance of your model on the testing data, and compute the same metrics we computed earlier."""

y_pred_boosted = adb_model.predict(x_balanced_test)
print("Accuracy:",accuracy_score(y_balanced_test, y_pred_boosted),"\nPrecision:",precision_score(y_balanced_test, y_pred_boosted),"\nRecall:",recall_score(y_balanced_test, y_pred_boosted),"\nF1 Score:",f1_score(y_balanced_test, y_pred_boosted))

"""**BONUS:** One thing you can try on your own, is to tune the Hyperparameters of the Adaboost algorithm, and build an even more powerful model :)"""

param_grid = {'n_estimators': [10, 50, 100, 500], 'learning_rate':[0.0001, 0.001, 0.01, 0.1, 1.0]}

tuned_adb = GridSearchCV(AdaBoostClassifier(), param_grid, cv=5)

tuned_adb.fit(x_balanced_train, y_balanced_train)
print("Best Score is:",tuned_adb.best_score_,"\nBest Parameters are:",tuned_adb.best_params_)

"""**Trying our new Hyperparameters of the Adaboost Algorithm:**"""

final_model=AdaBoostClassifier(n_estimators=500, learning_rate=0.1)
final_model.fit(x_balanced_train, y_balanced_train)
y_pred_final = final_model.predict(x_balanced_test)
print("Accuracy:",accuracy_score(y_balanced_test, y_pred_final),"\nPrecision:",precision_score(y_balanced_test, y_pred_final),"\nRecall:",recall_score(y_balanced_test, y_pred_final),"\nF1 Score:",f1_score(y_balanced_test, y_pred_final))